{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/anaconda3/envs/bi_development\"  #--> '/opt/anaconda3/envs/bi_development/bin/java'\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/spark/spark-3.1.2-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:06.833709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(OE='AZFD', CLAIM_CODE='340004286', FECHA_PROCESO=datetime.datetime(2006, 12, 1, 0, 0), ACCIDENT_DATE=datetime.datetime(1992, 4, 1, 0, 0), REPORTING_DATE=datetime.datetime(1992, 4, 1, 0, 0), CURRENCY='EUR', GUARANTEE='30', PORTFOLIO_LOB='1020', RESERVING_LOB='1020_BI', G_PAID=0.0, G_SS=0.0, G_CASE=6000.0, G_CASE_RECOVERY=0.0, C_PAID=0.0, C_SS=0.0, C_CASE=0.0, FLAG='1', FIRST_CLOSE_DATE='', LATEST_CLOSE_DATE='', NATCAT='0', INSTRUMENTAL='0', CONSORCIABLE='0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import datetime\n",
    "\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('JupyterHub').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "dt_ini = datetime.datetime.now()\n",
    "\n",
    "df_sp = spark.read.parquet('/mnt/lnxsasdtcn/sasdata/DISCO_M/ALZFenix/local/FtpPCSas/history_azfd_nov2019_sas')\n",
    "\n",
    "dt_end = datetime.datetime.now()\n",
    "print(dt_end - dt_ini)\n",
    "\n",
    "df_sp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 536 µs, sys: 3.38 ms, total: 3.92 ms\n",
      "Wall time: 14.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(OE='AZFD', CLAIM_CODE='340004286', FECHA_PROCESO=datetime.datetime(2006, 12, 1, 0, 0), ACCIDENT_DATE=datetime.datetime(1992, 4, 1, 0, 0), REPORTING_DATE=datetime.datetime(1992, 4, 1, 0, 0), CURRENCY='EUR', GUARANTEE='30', PORTFOLIO_LOB='1020', RESERVING_LOB='1020_BI', G_PAID=0.0, G_SS=0.0, G_CASE=6000.0, G_CASE_RECOVERY=0.0, C_PAID=0.0, C_SS=0.0, C_CASE=0.0, FLAG='1', FIRST_CLOSE_DATE='', LATEST_CLOSE_DATE='', NATCAT='0', INSTRUMENTAL='0', CONSORCIABLE='0', PROCESS_MONTH='200612')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_month = udf(lambda x:x.strftime('%Y%m') )\n",
    "%time df_month = df_sp.withColumn(\"PROCESS_MONTH\", convert_to_month(df_sp[\"FECHA_PROCESO\"]))\n",
    "\n",
    "#At the end, this timezones conversion aren't needed. Pyspark takes the right timezone from the parquet file by default.\n",
    "df_2 = df_month.withColumn(\"PROCESS_MONTH\", convert_to_month(from_utc_timestamp(split(df_month[\"FECHA_PROCESO\"],'\\+')[0],'GMT-1')))\n",
    "df_2 = df_2.withColumn(\"FECHA_PROCESO\", from_utc_timestamp(split(df_2[\"FECHA_PROCESO\"],'\\+')[0],'GMT-1')) \n",
    "df_2 = df_2.withColumn(\"ACCIDENT_DATE\", from_utc_timestamp(split(df_2[\"ACCIDENT_DATE\"],'\\+')[0],'GMT-2')) \n",
    "df_2 = df_2.withColumn(\"REPORTING_DATE\", from_utc_timestamp(split(df_2[\"REPORTING_DATE\"],'\\+')[0],'GMT-2')) \n",
    "\n",
    "\n",
    "df_month.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_month.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 µs, sys: 0 ns, total: 46 µs\n",
      "Wall time: 49.1 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('OE', 'string'),\n",
       " ('CLAIM_CODE', 'string'),\n",
       " ('FECHA_PROCESO', 'timestamp'),\n",
       " ('ACCIDENT_DATE', 'timestamp'),\n",
       " ('REPORTING_DATE', 'timestamp'),\n",
       " ('CURRENCY', 'string'),\n",
       " ('GUARANTEE', 'string'),\n",
       " ('PORTFOLIO_LOB', 'string'),\n",
       " ('RESERVING_LOB', 'string'),\n",
       " ('G_PAID', 'double'),\n",
       " ('G_SS', 'double'),\n",
       " ('G_CASE', 'double'),\n",
       " ('G_CASE_RECOVERY', 'double'),\n",
       " ('C_PAID', 'double'),\n",
       " ('C_SS', 'double'),\n",
       " ('C_CASE', 'double'),\n",
       " ('FLAG', 'string'),\n",
       " ('FIRST_CLOSE_DATE', 'string'),\n",
       " ('LATEST_CLOSE_DATE', 'string'),\n",
       " ('NATCAT', 'string'),\n",
       " ('INSTRUMENTAL', 'string'),\n",
       " ('CONSORCIABLE', 'string'),\n",
       " ('PROCESS_MONTH', 'string')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_month.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(OE,StringType,true),StructField(CLAIM_CODE,StringType,true),StructField(FECHA_PROCESO,TimestampType,true),StructField(ACCIDENT_DATE,TimestampType,true),StructField(REPORTING_DATE,TimestampType,true),StructField(CURRENCY,StringType,true),StructField(GUARANTEE,StringType,true),StructField(PORTFOLIO_LOB,StringType,true),StructField(RESERVING_LOB,StringType,true),StructField(G_PAID,DoubleType,true),StructField(G_SS,DoubleType,true),StructField(G_CASE,DoubleType,true),StructField(G_CASE_RECOVERY,DoubleType,true),StructField(C_PAID,DoubleType,true),StructField(C_SS,DoubleType,true),StructField(C_CASE,DoubleType,true),StructField(FLAG,StringType,true),StructField(FIRST_CLOSE_DATE,StringType,true),StructField(LATEST_CLOSE_DATE,StringType,true),StructField(NATCAT,StringType,true),StructField(INSTRUMENTAL,StringType,true),StructField(CONSORCIABLE,StringType,true),StructField(PROCESS_MONTH,StringType,true)))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_month.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|PROCESS_MONTH| count|\n",
      "+-------------+------+\n",
      "|       200206| 32035|\n",
      "|       201103| 68401|\n",
      "|       201809|156970|\n",
      "|       200606| 43669|\n",
      "|       201503| 80721|\n",
      "|       201607|129829|\n",
      "|       200203| 23851|\n",
      "|       201903| 90771|\n",
      "|       200106| 43229|\n",
      "|       201702| 77188|\n",
      "|       201510|149513|\n",
      "|       201909|150890|\n",
      "|       201611|173475|\n",
      "|       200209| 39162|\n",
      "|       201801| 66089|\n",
      "|       201808|146777|\n",
      "|       201009|117035|\n",
      "|       200706| 66218|\n",
      "|       201306| 98067|\n",
      "|       200809| 97504|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 18.6 ms, sys: 10.1 ms, total: 28.7 ms\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%time df_month.groupBy(\"PROCESS_MONTH\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInRead\", \"CORRECTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [x[0] for x in df_month.select(\"PROCESS_MONTH\").distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4:46:41.561663\n"
     ]
    }
   ],
   "source": [
    "dt_ini = datetime.datetime.now()   \n",
    "\n",
    "groups = [x[0] for x in df_month.select(\"PROCESS_MONTH\").distinct().collect()]\n",
    "for month in groups:\n",
    "    df_month.filter(df_month[\"PROCESS_MONTH\"]==month).\\\n",
    "    repartition(1).write.csv(r'data/fenix_file_{}'.format(month),\\\n",
    "                             compression=\"gzip\", mode=\"overwrite\", header=True,sep = ';')\n",
    "\n",
    "dt_end = datetime.datetime.now() \n",
    "\n",
    "print(dt_end - dt_ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
